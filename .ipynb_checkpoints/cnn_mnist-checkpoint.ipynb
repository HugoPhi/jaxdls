{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fd892b-57ab-4004-b79c-42f5f04f8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnsit at: /home/tibless/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit, random, lax\n",
    "\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")  # download mnist\n",
    "key = random.PRNGKey(239)\n",
    "print(f'mnsit at: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a01448a-876b-4ef6-8e21-e7392d93ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练图像数据大小:  (6000, 28, 28)\n",
      "训练标签数据大小:  (6000,)\n",
      "测试图像数据大小:  (1000, 28, 28)\n",
      "测试标签数据大小:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# load mnist\n",
    "\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def read_idx(filename):\n",
    "    \"\"\"\n",
    "    Read MNIST data from file using idx format.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    np.ndarray\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "TRAIN = 6000\n",
    "TEST  = 1000\n",
    "train_images = read_idx(os.path.join(path, 'train-images.idx3-ubyte'))[:TRAIN]\n",
    "train_labels = read_idx(os.path.join(path, 'train-labels.idx1-ubyte'))[:TRAIN]\n",
    "test_images = read_idx(os.path.join(path, 't10k-images.idx3-ubyte'))[:TEST]\n",
    "test_labels = read_idx(os.path.join(path, 't10k-labels.idx1-ubyte'))[:TEST]\n",
    "\n",
    "shuffle_kernel = np.random.permutation(np.arange(TRAIN))\n",
    "train_images = train_images[shuffle_kernel]\n",
    "train_labels = train_labels[shuffle_kernel]\n",
    "\n",
    "print(\"训练图像数据大小: \", train_images.shape)\n",
    "print(\"训练标签数据大小: \", train_labels.shape)\n",
    "print(\"测试图像数据大小: \", test_images.shape)\n",
    "print(\"测试标签数据大小: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523dc86d-6e89-4f57-8956-675bf9f52a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAADrJJREFUeJzt3H2s1/P/x/HnKaKL1Uo7FhmduVjNmegsF8t0xVnzRaYZs9GMP2ITczHRlZlhc9GSLlx0YfkHK0yULTX/tEJDTBNKWNdSKF2+v3/8fnv++BXO+9M5nVPdbn/x2efR55Wduns7elUVRVEEAEREq+Y+AAAthygAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkihwVFqzZk1UVVXFU0891Wg/5uLFi6OqqioWL17caD8mtDSiQIsxc+bMqKqqio8//ri5j9Ik5s6dG/X19XHKKafECSecEN27d49hw4bFF1980dxHg3Rccx8AjhUrVqyIzp07x8iRI6Nr166xfv36mD59evTt2zeWLFkS5513XnMfEUQBDpexY8ce8Nptt90W3bt3jylTpsTUqVOb4VTwV/7zEUeU3bt3x9ixY6NPnz7RqVOnaN++fVx66aWxaNGiv908++yzcfrpp0fbtm3jsssuO+h/rlm5cmUMGzYsunTpEieeeGLU1dXF22+//a/n2bFjR6xcuTI2b95c0c+nuro62rVrF7/88ktFe2hsosARZfv27fHSSy9F//7948knn4zx48fHpk2bor6+Pj799NMD3v/KK6/ExIkT484774xRo0bFF198EQMHDowNGzbke7788su46KKL4quvvooHH3wwnn766Wjfvn0MHTo05s6d+4/nWbZsWfTs2TMmTZrU4J/DL7/8Eps2bYoVK1bEbbfdFtu3b49BgwY1eA9NqoAWYsaMGUVEFB999NHfvmfv3r3Frl27/vLa1q1bi5NPPrm49dZb87XVq1cXEVG0bdu2+PHHH/P1pUuXFhFR3HPPPfnaoEGDitra2uKPP/7I1/bv319ccsklxVlnnZWvLVq0qIiIYtGiRQe8Nm7cuAb/PM8555wiIoqIKDp06FCMHj262LdvX4P30JQ8KXBEad26dbRp0yYiIvbv3x8///xz7N27N+rq6mL58uUHvH/o0KFx6qmn5t/37ds3Lrzwwnj33XcjIuLnn3+ODz74IK6//vr49ddfY/PmzbF58+bYsmVL1NfXx6pVq+Knn3762/P0798/iqKI8ePHN/jnMGPGjJg/f35Mnjw5evbsGTt37ox9+/Y1eA9NyTeaOeLMmjUrnn766Vi5cmXs2bMnX+/Ro8cB7z3rrLMOeO3ss8+O1157LSIivvnmmyiKIsaMGRNjxow56Odt3LjxL2E5VBdffHH+9Q033BA9e/aMiGjUP1MBlRIFjiizZ8+O4cOHx9ChQ+P++++P6urqaN26dTz++OPx7bfflv7x9u/fHxER9913X9TX1x/0PWeeeeYhnfmfdO7cOQYOHBivvvqqKNAiiAJHlDfeeCNqampizpw5UVVVla+PGzfuoO9ftWrVAa99/fXXccYZZ0RERE1NTUREHH/88TF48ODGP3AD7Ny5M7Zt29Ysnw3/n+8pcERp3bp1REQURZGvLV26NJYsWXLQ97/55pt/+Z7AsmXLYunSpTFkyJCI+J//JbR///4xbdq0WLdu3QH7TZs2/eN5yvwvqRs3bjzgtTVr1sTChQujrq7uX/dwOHhSoMWZPn16zJ8//4DXR44cGf/5z39izpw5ce2118aVV14Zq1evjqlTp0avXr3it99+O2Bz5plnRr9+/WLEiBGxa9eumDBhQpx00knxwAMP5Huef/756NevX9TW1sbtt98eNTU1sWHDhliyZEn8+OOP8dlnn/3tWZctWxYDBgyIcePG/es3m2tra2PQoEHRu3fv6Ny5c6xatSpefvnl2LNnTzzxxBMN/wcETUgUaHGmTJly0NeHDx8ew4cPj/Xr18e0adNiwYIF0atXr5g9e3a8/vrrB72o7uabb45WrVrFhAkTYuPGjdG3b9+YNGlSdOvWLd/Tq1ev+Pjjj+ORRx6JmTNnxpYtW6K6ujrOP//8g/4p5EqNGDEi5s2bF/Pnz49ff/01qqur44orroiHHnooamtrG+1z4FBUFX9+DgfgmOZ7CgAkUQAgiQIASRQASKIAQBIFAFKD/5zCn68UAODI05A/geBJAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIA6bjmPgA0hXvvvbf0pk2bNqU3d999d+nNtm3bSm8iImbMmFHR7nB4//33S28++eSTJjgJh8qTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUlVRFEWD3lhV1dRn4Sh30UUXVbRbsGBB6U27du1Kb3yNV2779u2lN5dffnlFn+Uivco15Ld7TwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEjHNfcBaFydOnUqvbnmmmtKb6666qrSm0ovQOvQoUPpTQPveaSRdOzYsfRm8ODBFX2WC/GalicFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkF+IdZbp161Z6M3369CY4SfP67rvvSm8+//zzJjjJgb755puKdjNmzCi9mThxYunNoEGDSm84enhSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkltSjzI7duwovVm4cGETnKTxPPzww6U3mzZtKr35/vvvS28Op06dOpXetG3btglOwtHMkwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIL8Y4ya9euLb2pr69vgpPwdy644IKKdpMnTy69qaurq+izytq3b1/pzbp165rgJBwqTwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEguxINDUFNTU3pz3333VfRZh+tyuy1btpTe3HrrraU38+bNK72h6XlSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAciEeR6UOHTqU3gwYMKD0ZtasWaU3HTt2LL2p1FdffVV6M378+NIbl9sdPTwpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyS2pVOTEE08svWnTpk1Fn/XYY4+V3px77rmlN5deemnpTSX27t1b0e7rr78uvbn66qtLb9asWVN6w9HDkwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAFJVURRFg95YVdXUZ+EIMnHixNKbO+64o6LPquRrr4Ff1s1i4cKFFe3q6+sb+SQcaxry68KTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0nHNfQCOTJVcUlfppYqtWpX/d5f9+/dX9FmHQ48ePSra3XLLLaU3s2bNquizOHZ5UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHIhHhX58MMPS28qvaSukov0iqIovRk+fHjpTfv27UtvampqSm8iIl544YXSmy1btpTevPPOO6U3HD08KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIFUVDbw5rJJLyeBIMmTIkNKb2bNnl9506tSp9KZS1113XenNW2+91QQnoSVoyG/3nhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYB0XHMfAFqK9957r/Rm/fr1pTeH85ZUKMuTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkgvx4H9169at9KZdu3ZNcBJoPp4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQjukL8U477bTSmx9++KEJTkJjO/nkk0tv3njjjdKbSr6GoCXzpABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNTiLsTr2rVrRbvnnnuu9KZ3796lN5Vcmvbiiy+W3qxdu7b0pqWrrq6uaNelS5fSm5kzZ5be1NXVld5U4vfff69ot2TJktKbZcuWVfRZHLs8KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIFUVRVE06I1VVU19loiIGDVqVEW7Rx99tJFP0njWrVtXerNw4cImOEnzquQCwoiI2tra0psGflkfsg0bNpTePPzwwxV9ViWX/MGfNeTXhScFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgtbhbUvv06VPR7q677iq9ufHGG0tvWrXS0cOtkq+97du3l94sX7689GbYsGGlN1u3bi29gcbgllQAShEFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDU4i7EO5zuvPPO0pu+ffuW3tx0002lN/yf0aNHl94888wzpTe7d+8uvYEjiQvxAChFFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0jF9IR7AscSFeACUIgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAOm4hr6xKIqmPAcALYAnBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDSfwEvrne5r5atjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ix = 4\n",
    "plt.imshow(train_images[ix], cmap='gray')\n",
    "plt.title(f\"Label: {train_labels[ix]}\")\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3655f999-7413-4003-b339-182e88e96585",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = jnp.array(train_images.reshape(train_images.shape[0], 1, 28, 28)) / 255.\n",
    "y_train = jnp.array(train_labels)\n",
    "X_test = jnp.array(test_images.reshape(test_images.shape[0], 1, 28, 28)) / 255.\n",
    "y_test = jnp.array(test_labels)\n",
    "\n",
    "def one_hot(y: jnp.ndarray, num_class: int):\n",
    "    res = jnp.zeros((y.shape[0], num_class))\n",
    "    res = res.at[jnp.arange(y.shape[0]), y].set(1)\n",
    "    return res\n",
    "\n",
    "y_train_one_hot = one_hot(y_train, 10)\n",
    "y_test_one_hot = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d3b0f3-1059-4411-8c1a-c243fe13becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax & Loss, version without improvement: \n",
    "# softmax = lambda logits: jnp.exp(logits) / jnp.sum(jnp.exp(logits), axis=1, keepdims=True)\n",
    "# cross_entropy_loss = lambda y, y_pred: jnp.sum(-y * jnp.log(y_pred), axis=1).mean()\n",
    "\n",
    "def softmax(logits):\n",
    "    logits_stable = logits - jnp.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = jnp.exp(logits_stable)\n",
    "    return exp_logits / jnp.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y, y_pred):\n",
    "    epsilon = 1e-9\n",
    "    y_pred_clipped = jnp.clip(y_pred, epsilon, 1. - epsilon)  # clip here is very important, or you will get Nan when you training. \n",
    "    loss = -jnp.sum(y * jnp.log(y_pred_clipped), axis=1)\n",
    "    return loss.mean()\n",
    "\n",
    "class RawVersion:\n",
    "    @staticmethod\n",
    "    def conv2d(x, w, b, padding=1):\n",
    "        bs, icl, he, wi = x.shape  # input graph -> batch_size x channel x height x width\n",
    "        ocl, icl, kh, kw = w.shape\n",
    "        he = (he + 2*padding - kh + 1)\n",
    "        wi = (wi + 2*padding - kw + 1)\n",
    "        \n",
    "        fgraph = jnp.zeros((bs, ocl, he, wi))  # feature graph\n",
    "    \n",
    "        # padding for x \n",
    "        pad_mat = (\n",
    "            (0, 0),\n",
    "            (0, 0),\n",
    "            (padding, padding),\n",
    "            (padding, padding)\n",
    "        )\n",
    "        \n",
    "        x_padded = jnp.pad(x, pad_mat, mode='constant', constant_values=0)\n",
    "    \n",
    "        for k in range(ocl):\n",
    "            for i in range(he):\n",
    "                for j in range(wi):\n",
    "                    fgraph.at[:, k, i, j].set(\n",
    "                        jnp.sum(x_padded[:, :, i:i + kh, j:j + kw] * w[k], axis=(1, 2, 3)) + b[k]\n",
    "                    )\n",
    "    \n",
    "        return fgraph\n",
    "\n",
    "    @staticmethod\n",
    "    def max_pooling2d(x, pool_size=(2, 2), stride=None):\n",
    "        if stride is None:\n",
    "            stride = pool_size\n",
    "        \n",
    "        batch_size, channels, height, width = x.shape\n",
    "        pool_height, pool_width = pool_size\n",
    "        stride_height, stride_width = stride\n",
    "        \n",
    "        output_height = (height - pool_height) // stride_height + 1\n",
    "        output_width = (width - pool_width) // stride_width + 1\n",
    "        \n",
    "        output_array = jnp.zeros((batch_size, channels, output_height, output_width))\n",
    "        \n",
    "        for n in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        window = x[n, c, \n",
    "                                        i * stride_height:i * stride_height + pool_height, \n",
    "                                        j * stride_width :j * stride_width  + pool_width]\n",
    "                        output_array.at[n, c, i, j].set(\n",
    "                            jnp.max(window)\n",
    "                        )\n",
    "        \n",
    "        return output_array        \n",
    "\n",
    "\n",
    "class JaxOptimaized:\n",
    "    @staticmethod\n",
    "    def conv2d(x, w, b, padding=1):\n",
    "        dimension_numbers = ('NCHW', 'OIHW', 'NCHW')\n",
    "        padding_mode = ((padding, padding), (padding, padding))  # 高度和宽度方向的padding\n",
    "        \n",
    "        out = lax.conv_general_dilated(\n",
    "            lhs=x,\n",
    "            rhs=w,\n",
    "            window_strides=(1, 1),\n",
    "            padding=padding_mode,\n",
    "            lhs_dilation=(1, 1),\n",
    "            rhs_dilation=(1, 1),\n",
    "            dimension_numbers=dimension_numbers\n",
    "        )\n",
    "        \n",
    "        return out + b[None, :, None, None]\n",
    "\n",
    "    @staticmethod\n",
    "    def max_pooling2d(x, pool_size=(2, 2), stride=None):\n",
    "        if stride is None:\n",
    "            stride = pool_size\n",
    "        \n",
    "        return lax.reduce_window(\n",
    "            operand=x,\n",
    "            init_value=-jnp.inf,\n",
    "            computation=lax.max,\n",
    "            window_dimensions=(1, 1, pool_size[0], pool_size[1]),\n",
    "            window_strides=(1, 1, stride[0], stride[1]),\n",
    "            padding='VALID'\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97727ee4-02b6-4880-b5f9-884e99f660e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            'conv2d:0' : {\n",
    "                'input_channel': 1,\n",
    "                'output_channel': 6,\n",
    "                'kernel_size': 5,\n",
    "                'padding': 2,\n",
    "            },\n",
    "            'max_pooling:0' : {\n",
    "                'window_size': 2,\n",
    "            },\n",
    "            'conv2d:1' : {\n",
    "                'input_channel': 6,\n",
    "                'output_channel': 16,\n",
    "                'kernel_size': 5,\n",
    "                'padding': 0,\n",
    "            },\n",
    "            'relu:1': {},\n",
    "            'max_pooling:1' : {\n",
    "                'window_size': 2,\n",
    "            },\n",
    "            'flatten:0': {},\n",
    "            'fc:0' : {\n",
    "                'input_dim': 400,\n",
    "                'output_dim': 120,\n",
    "            },\n",
    "            'relu:2': {},\n",
    "            'fc:1' : {\n",
    "                'input_dim': 120,\n",
    "                'output_dim': 84,\n",
    "            },\n",
    "            'relu:3': {},\n",
    "            'fc:2' : {\n",
    "                'input_dim': 84,\n",
    "                'output_dim': 10,\n",
    "            },\n",
    "            'relu:4': {}\n",
    "        }\n",
    "\n",
    "        self.params = {\n",
    "            'conv2d:0': {\n",
    "                'w': self._w_conv2d_init((\n",
    "                    self.config['conv2d:0']['output_channel'],\n",
    "                    self.config['conv2d:0']['input_channel'],\n",
    "                    self.config['conv2d:0']['kernel_size'],\n",
    "                    self.config['conv2d:0']['kernel_size'],\n",
    "                )),  # (O, I, KH, KW)\n",
    "                'b': jnp.zeros(self.config['conv2d:0']['output_channel']), # b: (O,)\n",
    "            },\n",
    "            'conv2d:1': {\n",
    "                'w': self._w_conv2d_init((\n",
    "                    self.config['conv2d:1']['output_channel'],\n",
    "                    self.config['conv2d:1']['input_channel'],\n",
    "                    self.config['conv2d:1']['kernel_size'],\n",
    "                    self.config['conv2d:1']['kernel_size'],\n",
    "                )),\n",
    "                'b': jnp.zeros(self.config['conv2d:1']['output_channel'])\n",
    "            },\n",
    "            'fc:0': {\n",
    "                'w': self._w_fc_init((\n",
    "                    self.config['fc:0']['input_dim'],\n",
    "                    self.config['fc:0']['output_dim'],\n",
    "                )),\n",
    "                'b': jnp.zeros(1),\n",
    "            },\n",
    "            'fc:1': {\n",
    "                'w': self._w_fc_init((\n",
    "                    self.config['fc:1']['input_dim'],\n",
    "                    self.config['fc:1']['output_dim'],\n",
    "                )),\n",
    "                'b': jnp.zeros(1),\n",
    "            },\n",
    "            'fc:2': {\n",
    "                'w': self._w_fc_init((\n",
    "                    self.config['fc:2']['input_dim'],\n",
    "                    self.config['fc:2']['output_dim'],\n",
    "                )),\n",
    "                'b': jnp.zeros(1),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.v_params = {k: {name: jnp.zeros_like(w) for name, w in v.items()} for k, v in self.params.items()}\n",
    "        self.vv_params = {k: {name: jnp.zeros_like(w) for name, w in v.items()} for k, v in self.params.items()}\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "    def _adam_update(self, gs: dict, params, v_params, vv_params,\n",
    "                    lr=0.01,\n",
    "                    beta1=0.9,\n",
    "                    beta2=0.999,\n",
    "                    epsilon=1e-6):\n",
    "\n",
    "        t = self.step + 1 # step of training, starting from 0, so add 1\n",
    "\n",
    "        names = gs.keys()\n",
    "        new_params, new_v_params, new_vv_params = [], [], []\n",
    "        for g, w, v_w, vv_w in zip(gs.values(), params.values(), v_params.values(), vv_params.values()):\n",
    "            new_w, new_v_w, new_vv_w = {}, {}, {}\n",
    "\n",
    "            for ix in ['w', 'b']:\n",
    "                new_v_w[ix] = beta1*v_w[ix] + (1 - beta1)*g[ix]\n",
    "                new_vv_w[ix] = beta2*vv_w[ix] + (1 - beta2)*g[ix]*g[ix]\n",
    "    \n",
    "                v_w_hat = new_v_w[ix] / (1 - beta1**t)\n",
    "                vv_w_hat = new_vv_w[ix] / (1 - beta2**t)\n",
    "                step = - lr * v_w_hat / (jnp.sqrt(vv_w_hat) + epsilon)\n",
    "\n",
    "                new_w[ix] = w[ix] + step\n",
    "\n",
    "            new_params.append(new_w)\n",
    "            new_v_params.append(new_v_w)\n",
    "            new_vv_params.append(new_vv_w)\n",
    "\n",
    "        return dict(zip(names, new_params)), dict(zip(names, new_v_params)), dict(zip(names, new_vv_params))\n",
    "       \n",
    "\n",
    "    def predict_proba(self, x: jnp.ndarray, params):\n",
    "        res = x\n",
    "        res = JaxOptimaized.conv2d(res, params['conv2d:0']['w'], params['conv2d:0']['b'], padding=self.config['conv2d:0']['padding'])\n",
    "        res = jnp.maximum(0, res)\n",
    "        res = JaxOptimaized.max_pooling2d(res, pool_size=(\n",
    "            self.config['max_pooling:0']['window_size'],\n",
    "            self.config['max_pooling:0']['window_size'],\n",
    "        ))\n",
    "        \n",
    "        res = JaxOptimaized.conv2d(res, params['conv2d:1']['w'], params['conv2d:1']['b'], padding=self.config['conv2d:1']['padding'])\n",
    "        res = jnp.maximum(0, res)\n",
    "        res = JaxOptimaized.max_pooling2d(res, pool_size=(\n",
    "            self.config['max_pooling:1']['window_size'],\n",
    "            self.config['max_pooling:1']['window_size'],\n",
    "        ))\n",
    "\n",
    "        res = res.reshape(res.shape[0], -1)\n",
    "\n",
    "        res = res @ params['fc:0']['w'] + params['fc:0']['b']\n",
    "        res = jnp.maximum(0, res)\n",
    "        \n",
    "        res = res @ params['fc:1']['w'] + params['fc:1']['b']\n",
    "        res = jnp.maximum(0, res)\n",
    "        \n",
    "        res = res @ params['fc:2']['w'] + params['fc:2']['b']\n",
    "        res = jnp.maximum(0, res)\n",
    "\n",
    "        res = softmax(res)\n",
    "        return res\n",
    "       \n",
    "    def _w_fc_init(self, w_shape):\n",
    "        w = random.normal(key, w_shape) * jnp.sqrt(2 / (w_shape[0]))  # kaiming: sqrt(2 / Input_dim)\n",
    "        return w\n",
    "\n",
    "    def _w_conv2d_init(self, w_shape):\n",
    "        w = random.normal(key, w_shape) * jnp.sqrt(2 / (w_shape[0] * w_shape[1] * w_shape[2]))  # kaiming: sqrt(2 / I x KH x KH)\n",
    "        return w\n",
    " \n",
    "    def fit(self, x_train, y_train_proba, x_test, y_test_proba, \n",
    "            epoches=100, \n",
    "            lr=0.1,\n",
    "            beta1=0.9,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-6): \n",
    "        \n",
    "        def _acc(y_true_proba, y_pred_proba):\n",
    "            y_true = jnp.argmax(y_true_proba, axis=1)\n",
    "            y_pred = jnp.argmax(y_pred_proba, axis=1)\n",
    "            return np.mean(y_true == y_pred)\n",
    "\n",
    "        _loss = lambda params: cross_entropy_loss(y_train_proba, softmax(self.predict_proba(x_train, params))) \n",
    "        _loss = jit(_loss)  # accelerate loss function by JIT\n",
    "        \n",
    "        _tloss = lambda params: cross_entropy_loss(y_test_proba, softmax(self.predict_proba(x_test, params))) \n",
    "        \n",
    "        acc, loss, tacc, tloss = [], [], [], []  # train acc, train loss, test acc, test loss\n",
    "        \n",
    "        for _ in range(epoches):\n",
    "            loss.append(_loss(self.params))\n",
    "            tloss.append(_tloss(self.params))\n",
    "\n",
    "            d_params = grad(_loss, argnums=0)(self.params)\n",
    "\n",
    "            # Adam\n",
    "            self.params, self.v_params, self.vv_params = self._adam_update(d_params, self.params, self.v_params, self.vv_params, lr, beta1, beta2, epsilon)\n",
    "            \n",
    "            self.step += 1  # remember update step, all variants updated.\n",
    "            \n",
    "            acc.append(_acc(y_train_proba, self.predict_proba(x_train, self.params)))\n",
    "            tacc.append(_acc(y_test_proba, self.predict_proba(x_test, self.params)))\n",
    "            if self.step % 10 == 0:\n",
    "                print(f'>> epoch: {self.step}, train acc: {acc[-1]}, test acc: {tacc[-1]}')\n",
    "\n",
    "        return acc, loss, tacc, tloss       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf14990-0c34-4c07-8f34-80db6e1b6e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 10, train acc: 0.35883334279060364, test acc: 0.359000027179718\n",
      ">> epoch: 20, train acc: 0.4026666581630707, test acc: 0.3920000195503235\n",
      ">> epoch: 30, train acc: 0.40950000286102295, test acc: 0.3960000276565552\n",
      ">> epoch: 40, train acc: 0.41333332657814026, test acc: 0.39900001883506775\n",
      ">> epoch: 50, train acc: 0.41616666316986084, test acc: 0.39900001883506775\n",
      ">> epoch: 60, train acc: 0.4166666567325592, test acc: 0.39900001883506775\n",
      ">> epoch: 70, train acc: 0.43199998140335083, test acc: 0.41200003027915955\n",
      ">> epoch: 80, train acc: 0.4988333284854889, test acc: 0.46800002455711365\n",
      ">> epoch: 90, train acc: 0.6031666398048401, test acc: 0.5730000138282776\n",
      ">> epoch: 100, train acc: 0.668999969959259, test acc: 0.6360000371932983\n",
      ">> epoch: 110, train acc: 0.7770000100135803, test acc: 0.7330000400543213\n",
      ">> epoch: 120, train acc: 0.878166675567627, test acc: 0.8270000219345093\n",
      ">> epoch: 130, train acc: 0.9399999976158142, test acc: 0.9220000505447388\n",
      ">> epoch: 140, train acc: 0.9810000061988831, test acc: 0.9610000252723694\n",
      ">> epoch: 150, train acc: 0.9891666769981384, test acc: 0.9720000624656677\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model = LeNet()\n",
    "acc, loss, tacc, tloss = model.fit(\n",
    "    x_train=X_train, \n",
    "    y_train_proba=y_train_one_hot,\n",
    "    x_test=X_test,\n",
    "    y_test_proba=y_test_one_hot,\n",
    "    epoches=epochs,\n",
    "    lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e31011-8930-403f-b12f-0909493fa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.plot(range(epochs), acc, color=color, label='Train Accuracy', linestyle='-')\n",
    "ax1.plot(range(epochs), tacc, color=color, label='Test Accuracy', linestyle='--')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Loss', color=color)  \n",
    "ax2.plot(range(epochs), loss, color=color, label='Train Loss', linestyle='-')\n",
    "ax2.plot(range(epochs), tloss, color=color, label='Test Loss', linestyle='--')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handles1 + handles2, labels1 + labels2, loc='lower right')\n",
    "\n",
    "plt.title('Training and Testing Accuracy and Loss over Epochs')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'final train, test acc : {acc[-1]}, {tacc[-1]}')\n",
    "print(f'final train, test loss: {loss[-1]}, {tloss[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
