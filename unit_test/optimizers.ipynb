{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40f8dbc-de96-4f24-b9d1-ffcc353900fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit, random, tree\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34583cf5-3a39-4890-bffe-f0837d0522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizter(ABC):\n",
    "    '''\n",
    "    A Class included by model; regarded as a container for weights & update weights by steps.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def open(self, _loss):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "    def get_setps(self):\n",
    "        return self.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "275247f8-981f-404d-bd49-f90563d04ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    logits_stable = logits - jnp.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = jnp.exp(logits_stable)\n",
    "    return exp_logits / jnp.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y, y_pred):\n",
    "    epsilon = 1e-9\n",
    "    y_pred_clipped = jnp.clip(y_pred, epsilon, 1. - epsilon)  # clip here is very important, or you will get Nan when you training. \n",
    "    loss = -jnp.sum(y * jnp.log(y_pred_clipped), axis=1)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "class Adam(Optimizter):\n",
    "    def __init__(self, params, \n",
    "                 lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-6):\n",
    "        self.params = params\n",
    "\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def flash(self):\n",
    "        self.V = tree.map(lambda x: jnp.zeros_like(x), self.params)\n",
    "        self.VV = tree.map(lambda x: jnp.zeros_like(x), self.params)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.open = False\n",
    "\n",
    "    def open(self, _loss):\n",
    "        '''\n",
    "        Input\n",
    "        -----\n",
    "        x_tarin: training set input\n",
    "        y_train: training set label\n",
    "        '''\n",
    "\n",
    "        if self.open is True:\n",
    "            print('oprimizer is already opened.')\n",
    "        else:\n",
    "            self.flash()\n",
    "            self._loss = _loss\n",
    "            self.open = True\n",
    "\n",
    "    def close(self):\n",
    "        if self.open is False:\n",
    "            print('oprimizer is already closed.')\n",
    "        else: \n",
    "            self.open = False\n",
    "    \n",
    "    def update(self):\n",
    "        if self.open is False:\n",
    "            raise ValueError('please open optimizer first!!!')\n",
    "        else:\n",
    "            d_params = grad(self._loss, argnums=0)(self.params)\n",
    "    \n",
    "            t = self.steps + 1\n",
    "    \n",
    "            def adam(d_w, w, v, vv):\n",
    "                v = self.beta1*v + (1 - self.beta1)*d_w\n",
    "                vv = self.beta2*vv + (1 - self.beta2)*d_w*d_w\n",
    "    \n",
    "                v_hat = v / (1 - self.beta1**t)\n",
    "                vv_hat = vv / (1 - self.beta2**t)\n",
    "                step = - self.lr * v_hat / (jnp.sqrt(vv_hat) + self.epsilon)\n",
    "    \n",
    "                w = w + step\n",
    "                return jnp.stack((\n",
    "                    w,\n",
    "                    v,\n",
    "                    vv,\n",
    "                ))\n",
    "    \n",
    "            def decode(pack, num_return=3):\n",
    "                res = []\n",
    "                for i in range(num_return):\n",
    "                    res.append(tree.map(lambda x: x[i], pack))\n",
    "\n",
    "                return res\n",
    "    \n",
    "            pack = tree.map(adam, d_params, self.params, self.V, self.VV)\n",
    "            self.params, self.V, self.VV = decode(pack)\n",
    "            self.steps += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
